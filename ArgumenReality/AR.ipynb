{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "328faf4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(196, 196, 3)\n",
      "257 234 196 196\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 3 is out of bounds for axis 2 with size 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 67\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# Run the main function\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 67\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 41\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     38\u001b[0m y_offset \u001b[38;5;241m=\u001b[39m y \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mint\u001b[39m(h\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Calculate the alpha channels for blending\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m alpha_s \u001b[38;5;241m=\u001b[39m \u001b[43mresized_object\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m\n\u001b[0;32m     42\u001b[0m alpha_l \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m alpha_s\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Blend the virtual object with the frame\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 3 is out of bounds for axis 2 with size 3"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# Load the cascade for face detection\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Load the image of the virtual object (hat)\n",
    "virtual_object = cv2.imread('hat.jpeg', -1)\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    # Open the webcam\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    while True:\n",
    "        # Read the video stream\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        # Convert the frame to grayscale\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Detect faces in the frame\n",
    "        faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "        \n",
    "        # Iterate over each detected face\n",
    "        for (x, y, w, h) in faces:\n",
    "            \n",
    "            # Check if the dimensions of the detected face are valid and non-empty\n",
    "            if w > 0 and h > 0:\n",
    "                # Verify that the virtual object is not empty\n",
    "                if virtual_object is not None:\n",
    "                    # Resize the virtual object to match the size of the detected face\n",
    "                    resized_object = cv2.resize(virtual_object, (w, h))\n",
    "                    print(resized_object.shape)\n",
    "                    print(x, y, w, h)\n",
    "                    \n",
    "                    # Calculate the position to place the virtual object on the face\n",
    "                    x_offset = x\n",
    "                    y_offset = y - int(h/2)\n",
    "                    \n",
    "                    # Calculate the alpha channels for blending\n",
    "                    alpha_s = resized_object[:, :, 3] / 255.0\n",
    "                    alpha_l = 1.0 - alpha_s\n",
    "                    \n",
    "                    # Blend the virtual object with the frame\n",
    "                    for c in range(0, 3):\n",
    "                        frame[y_offset:y_offset+h, x_offset:x_offset+w, c] = (alpha_s * resized_object[:, :, c] +\n",
    "                                                                               alpha_l * frame[y_offset:y_offset+h,\n",
    "                                                                                               x_offset:x_offset+w, c])\n",
    "                else:\n",
    "                    print(\"Virtual object is empty\")\n",
    "            else:\n",
    "                print(\"Invalid face dimensions\")\n",
    "        \n",
    "        # Display the augmented frame\n",
    "        cv2.imshow('Augmented Reality', frame)\n",
    "        \n",
    "        # Exit loop when 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    # Release the video capture and close the window\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40a71fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Using cached opencv_python-4.7.0.72-cp37-abi3-win_amd64.whl (38.2 MB)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\yahya\\anaconda3\\lib\\site-packages (from opencv-python) (1.21.5)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.7.0.72\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce57b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Load the cascade for face detection\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Load the image of the virtual object (hat)\n",
    "virtual_object = cv2.imread('hat.jpeg')\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    # Open the webcam\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    while True:\n",
    "        # Read the video stream\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        # Convert the frame to grayscale\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Detect faces in the frame\n",
    "        faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "        \n",
    "        # Iterate over each detected face\n",
    "        for (x, y, w, h) in faces:\n",
    "            \n",
    "            # Check if the dimensions of the detected face are valid and non-empty\n",
    "            if w > 0 and h > 0:\n",
    "                # Verify that the virtual object is not empty\n",
    "                if virtual_object is not None:\n",
    "                    # Resize the virtual object to match the size of the detected face\n",
    "                    resized_object = cv2.resize(virtual_object, (w, h))\n",
    "                    print(resized_object.shape)\n",
    "                    print(x, y, w, h)\n",
    "                    \n",
    "                    # Calculate the position to place the virtual object on the face\n",
    "                    x_offset = x\n",
    "                    y_offset = y - int(h/2)\n",
    "                    \n",
    "                    # Blend the virtual object with the frame\n",
    "                    frame_height, frame_width, _ = frame.shape\n",
    "                    for i in range(resized_object.shape[0]):\n",
    "                        for j in range(resized_object.shape[1]):\n",
    "                            if y_offset+i >= 0 and y_offset+i < frame_height and x_offset+j >= 0 and x_offset+j < frame_width:\n",
    "                                if resized_object[i, j, 0] != 0 and resized_object[i, j, 1] != 0 and resized_object[i, j, 2] != 0:\n",
    "                                    frame[y_offset+i, x_offset+j] = resized_object[i, j]\n",
    "                else:\n",
    "                    print(\"Virtual object is empty\")\n",
    "            else:\n",
    "                print(\"Invalid face dimensions\")\n",
    "        \n",
    "        # Display the augmented frame\n",
    "        cv2.imshow('Augmented Reality', frame)\n",
    "        \n",
    "        # Exit loop when 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    # Release the video capture and close the window\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vp": {
   "vp_config_version": "1.0.0",
   "vp_menu_width": 273,
   "vp_note_display": false,
   "vp_note_width": 0,
   "vp_position": {
    "width": 278
   },
   "vp_section_display": false,
   "vp_signature": "VisualPython"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
